{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Bias-Variance Tradeoff\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Agenda\n", "\n", "1. Revisit the goal of model building, and relate it to expected value, bias and variance\n", "2. Defining Error: prediction error and irreducible error\n", "3. Define prediction error as a combination of bias and variance\n", "4. Explore the bias-variance tradeoff\n", "5. Code a basic train-test split\n", "6. Code K-Folds\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Revisit the goal of model building, and relate it to expected value, bias and variance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![which model is better](img/which_model_is_better.png)\n", "\n", "https://towardsdatascience.com/cultural-overfitting-and-underfitting-or-why-the-netflix-culture-wont-work-in-your-company-af2a62e41288\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# What makes a model good?\n", "\n", "- We don\u2019t ultimately care about how well your model fits your data.\n", "\n", "- What we really care about is how well your model describes the process that generated your data.\n", "\n", "- Why? Because the data set you have is but one sample from a universe of possible data sets, and you want a model that would work for any data set from that universe"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# What is a \u201cModel\u201d?\n", "\n", " - A \u201cmodel\u201d is a general specification of relationships among variables. \n", "     - E.G. Linear Regression: or $ Price = \\beta_1*Time +  \\beta_0 + \\epsilon$\n", "\n", "\n", " "]}, {"cell_type": "markdown", "metadata": {}, "source": [" - A \u201ctrained model\u201d is a particular model with parameters estimated using some training data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Remember Expected Value? How is it connected to bias and variance?\n", "- The expected value of a quantity is the weighted average of that quantity across all possible samples\n", "\n", "![6 sided die](https://media.giphy.com/media/sRJdpUSr7W0AiQ3RcM/giphy.gif)\n", "\n", "- for a 6 sided die, another way to think about the expected value is the arithmetic mean of the rolls of a very large number of independent samples.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The expected value of a 6-sided die is:"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"data": {"text/plain": ["3.5"]}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": ["probs = 1/6\n", "rolls = range(1,7)\n", "\n", "expected_value = sum([probs * roll for roll in rolls])\n", "expected_value"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Now lets imagine we create a model that always predicts a roll of 3.\n", "\n", "    - The bias is the difference between the average prediction of our model and the average roll of the die as we roll more and more times.\n", "        - What is the bias of a model that alway predicts 3? \n", "   \n", "   \n", "    - The variance is the average difference between each individual prediction and the average prediction of our model as we roll more and more times.\n", "        - What is the variance of that model?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Defining Error: prediction error and irreducible error\n", "\n", "\n", "\n", "### Regression fit statistics are often called \u201cerror\u201d\n", " - Sum of Squared Errors (SSE)\n", " $ {\\displaystyle \\operatorname {SSE} =\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n", " - Mean Squared Error (MSE) \n", " \n", " $ {\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n", " \n", " - Root Mean Squared Error (RMSE)  \n", " $ {\\displaystyle \\operatorname \n", "  {RMSE} =\\sqrt{MSE}} $\n", "\n", " All are calculated using residuals    \n", "\n", "![residuals](img/residuals.png)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Individual Code: Turn Off Screen\n", "\n", " - Fit a quick and dirty linear regression model\n", " - Store predictions in the y_hat variable using predict() from the fit model\n", " - handcode SSE\n", " - Divide by the length of array to find Mean Squared Error\n", " - Make sure MSE equals sklearn's mean_squared_error function\n", " "]}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_squared_error\n", "np.random.seed(42)\n", "df = pd.read_csv('data/king_county.csv', index_col='id')\n", "df = df.iloc[:,:12]\n", "X = df.drop('price', axis=1)\n", "y = df.price\n", "\n", "y_hat = None\n", "sse = None\n", "mse = None\n", "rmse = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## This error can be broken up into parts:\n", "\n", "![defining error](img/defining_error.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There will always be some random, irreducible error inherent in the data.  Real data always has noise.\n", "\n", "The goal of modeling is to reduce the prediction error, which is the difference between our model and the realworld processes from which our data is generated."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Define prediction error as a combination of bias and variance\n", "\n", "$\\Large Total\\ Error\\ = Prediction\\ Error+ Irreducible\\ Error$\n", "\n", "Our prediction error can be further broken down into error due to bias and error due to variance.\n", "\n", "$\\Large Total\\ Error = Model\\ Bias^2 + Model\\ Variance + Irreducible\\ Error$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Model Bias** is the expected prediction error of the expected trained model\n", "\n", "> In other words, if you were to train multiple models on different samples, what would be the average difference between the prediction and the real value.\n", "\n", "**Model Variance** is the expected variation in predictions, relative to your expected trained model\n", "\n", "> In other words, what would be the average difference between any one model's prediction and the average of all the predictions .\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Thought Experiment\n", "\n", "1. Imagine you've collected 23 different training sets for the same problem.\n", "2. Now imagine using one algorithm to train 23 models, one for each of your training sets.\n", "3. Bias vs. variance refers to the accuracy vs. consistency of the models trained by your algorithm.\n", "\n", "![target_bias_variance](img/target.png)\n", "\n", "http://scott.fortmann-roe.com/docs/BiasVariance.html\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4.  Explore Bias Variance Tradeoff"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**High bias** algorithms tend to be less complex, with simple or rigid underlying structure.\n", "\n", "+ They train models that are consistent, but inaccurate on average.\n", "+ These include linear or parametric algorithms such as regression and naive Bayes.\n", "+ For linear, perhaps some assumptions about our feature set could lead to high bias. \n", "      - We did not include the correct predictors\n", "      - We did not take interactions into account\n", "      - In linear, we missed a non-linear relationship (polynomial). \n", "      \n", "High bias models are **underfit**\n", "\n", "On the other hand, **high variance** algorithms tend to be more complex, with flexible underlying structure.\n", "\n", "+ They train models that are accurate on average, but inconsistent.\n", "+ These include non-linear or non-parametric algorithms such as decision trees and nearest neighbors.\n", "+ For linear, perhaps we included an unreasonably large amount of predictors. \n", "      - We created new features by squaring and cubing each feature\n", "+ High variance models are modeling the noise in our data\n", "\n", "High variance models are **overfit**\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["While we build our models, we have to keep this relationship in mind.  If we build complex models, we risk overfitting our models.  Their predictions will vary greatly when introduced to new data.  If our models are too simple, the predictions as a whole will be inaccurate.   \n", "\n", "The goal is to build a model with enough complexity to be accurate, but not too much complexity to be erratic."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![optimal](img/optimal_bias_variance.png)\n", "http://scott.fortmann-roe.com/docs/BiasVariance.html\n", "\n", "### Let's take a look at our familiar King County housing data. "]}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>price</th>\n", "      <th>bedrooms</th>\n", "      <th>bathrooms</th>\n", "      <th>sqft_living</th>\n", "      <th>sqft_lot</th>\n", "      <th>floors</th>\n", "      <th>waterfront</th>\n", "      <th>view</th>\n", "      <th>condition</th>\n", "      <th>grade</th>\n", "      <th>sqft_above</th>\n", "      <th>sqft_basement</th>\n", "    </tr>\n", "    <tr>\n", "      <th>id</th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>7129300520</th>\n", "      <td>221900.0</td>\n", "      <td>3</td>\n", "      <td>1.00</td>\n", "      <td>1180</td>\n", "      <td>5650</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>7</td>\n", "      <td>1180</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6414100192</th>\n", "      <td>538000.0</td>\n", "      <td>3</td>\n", "      <td>2.25</td>\n", "      <td>2570</td>\n", "      <td>7242</td>\n", "      <td>2.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>7</td>\n", "      <td>2170</td>\n", "      <td>400</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5631500400</th>\n", "      <td>180000.0</td>\n", "      <td>2</td>\n", "      <td>1.00</td>\n", "      <td>770</td>\n", "      <td>10000</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>6</td>\n", "      <td>770</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2487200875</th>\n", "      <td>604000.0</td>\n", "      <td>4</td>\n", "      <td>3.00</td>\n", "      <td>1960</td>\n", "      <td>5000</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>5</td>\n", "      <td>7</td>\n", "      <td>1050</td>\n", "      <td>910</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1954400510</th>\n", "      <td>510000.0</td>\n", "      <td>3</td>\n", "      <td>2.00</td>\n", "      <td>1680</td>\n", "      <td>8080</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>8</td>\n", "      <td>1680</td>\n", "      <td>0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["               price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n", "id                                                                         \n", "7129300520  221900.0         3       1.00         1180      5650     1.0   \n", "6414100192  538000.0         3       2.25         2570      7242     2.0   \n", "5631500400  180000.0         2       1.00          770     10000     1.0   \n", "2487200875  604000.0         4       3.00         1960      5000     1.0   \n", "1954400510  510000.0         3       2.00         1680      8080     1.0   \n", "\n", "            waterfront  view  condition  grade  sqft_above  sqft_basement  \n", "id                                                                         \n", "7129300520           0     0          3      7        1180              0  \n", "6414100192           0     0          3      7        2170            400  \n", "5631500400           0     0          3      6         770              0  \n", "2487200875           0     0          5      7        1050            910  \n", "1954400510           0     0          3      8        1680              0  "]}, "execution_count": 49, "metadata": {}, "output_type": "execute_result"}], "source": ["import pandas as pd\n", "import numpy as np\n", "np.random.seed(42)\n", "df = pd.read_csv('data/king_county.csv', index_col='id')\n", "df = df.iloc[:,:12]\n", "df.head()"]}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_squared_error\n", "np.random.seed(42)\n", "\n", "# Let's generate random subsets of our data\n", "\n", "#Date  is not in the correct format so we are dropping it for now.\n", "\n", "r_2 = []\n", "simple_rmse = []\n", "\n", "for i in range(100):\n", "    \n", "    df_sample = df.sample(5000, replace=True)\n", "    y = df_sample.price\n", "    X = df_sample.drop('price', axis=1)\n", "    \n", "    lr = LinearRegression()\n", "    lr.fit(X, y)\n", "    \n", "    y_hat = lr.predict(X)\n", "    simple_rmse.append(np.sqrt(mean_squared_error(y, y_hat)))\n", "    r_2.append(lr.score(X,y))\n", "    \n", "    \n"]}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["simple mean 228303.4528251759\n", "simple variance 73238189.28935923\n"]}], "source": ["print(f'simple mean {np.mean(simple_rmse)}')\n", "print(f'simple variance {np.var(simple_rmse)}')"]}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>price</th>\n", "      <th>0</th>\n", "      <th>1</th>\n", "      <th>2</th>\n", "      <th>3</th>\n", "      <th>4</th>\n", "      <th>5</th>\n", "      <th>6</th>\n", "      <th>7</th>\n", "      <th>8</th>\n", "      <th>...</th>\n", "      <th>68</th>\n", "      <th>69</th>\n", "      <th>70</th>\n", "      <th>71</th>\n", "      <th>72</th>\n", "      <th>73</th>\n", "      <th>74</th>\n", "      <th>75</th>\n", "      <th>76</th>\n", "      <th>77</th>\n", "    </tr>\n", "    <tr>\n", "      <th>id</th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>7129300520</th>\n", "      <td>221900.0</td>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>1.00</td>\n", "      <td>1180.0</td>\n", "      <td>5650.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>21.0</td>\n", "      <td>3540.0</td>\n", "      <td>0.0</td>\n", "      <td>49.0</td>\n", "      <td>8260.0</td>\n", "      <td>0.0</td>\n", "      <td>1392400.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6414100192</th>\n", "      <td>538000.0</td>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>2.25</td>\n", "      <td>2570.0</td>\n", "      <td>7242.0</td>\n", "      <td>2.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>21.0</td>\n", "      <td>6510.0</td>\n", "      <td>1200.0</td>\n", "      <td>49.0</td>\n", "      <td>15190.0</td>\n", "      <td>2800.0</td>\n", "      <td>4708900.0</td>\n", "      <td>868000.0</td>\n", "      <td>160000.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5631500400</th>\n", "      <td>180000.0</td>\n", "      <td>1.0</td>\n", "      <td>2.0</td>\n", "      <td>1.00</td>\n", "      <td>770.0</td>\n", "      <td>10000.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>18.0</td>\n", "      <td>2310.0</td>\n", "      <td>0.0</td>\n", "      <td>36.0</td>\n", "      <td>4620.0</td>\n", "      <td>0.0</td>\n", "      <td>592900.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2487200875</th>\n", "      <td>604000.0</td>\n", "      <td>1.0</td>\n", "      <td>4.0</td>\n", "      <td>3.00</td>\n", "      <td>1960.0</td>\n", "      <td>5000.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>5.0</td>\n", "      <td>...</td>\n", "      <td>25.0</td>\n", "      <td>35.0</td>\n", "      <td>5250.0</td>\n", "      <td>4550.0</td>\n", "      <td>49.0</td>\n", "      <td>7350.0</td>\n", "      <td>6370.0</td>\n", "      <td>1102500.0</td>\n", "      <td>955500.0</td>\n", "      <td>828100.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1954400510</th>\n", "      <td>510000.0</td>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>2.00</td>\n", "      <td>1680.0</td>\n", "      <td>8080.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>24.0</td>\n", "      <td>5040.0</td>\n", "      <td>0.0</td>\n", "      <td>64.0</td>\n", "      <td>13440.0</td>\n", "      <td>0.0</td>\n", "      <td>2822400.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>7237550310</th>\n", "      <td>1225000.0</td>\n", "      <td>1.0</td>\n", "      <td>4.0</td>\n", "      <td>4.50</td>\n", "      <td>5420.0</td>\n", "      <td>101930.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>33.0</td>\n", "      <td>11670.0</td>\n", "      <td>4590.0</td>\n", "      <td>121.0</td>\n", "      <td>42790.0</td>\n", "      <td>16830.0</td>\n", "      <td>15132100.0</td>\n", "      <td>5951700.0</td>\n", "      <td>2340900.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1321400060</th>\n", "      <td>257500.0</td>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>2.25</td>\n", "      <td>1715.0</td>\n", "      <td>6819.0</td>\n", "      <td>2.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>21.0</td>\n", "      <td>5145.0</td>\n", "      <td>0.0</td>\n", "      <td>49.0</td>\n", "      <td>12005.0</td>\n", "      <td>0.0</td>\n", "      <td>2941225.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2008000270</th>\n", "      <td>291850.0</td>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>1.50</td>\n", "      <td>1060.0</td>\n", "      <td>9711.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>21.0</td>\n", "      <td>3180.0</td>\n", "      <td>0.0</td>\n", "      <td>49.0</td>\n", "      <td>7420.0</td>\n", "      <td>0.0</td>\n", "      <td>1123600.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2414600126</th>\n", "      <td>229500.0</td>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>1.00</td>\n", "      <td>1780.0</td>\n", "      <td>7470.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>21.0</td>\n", "      <td>3150.0</td>\n", "      <td>2190.0</td>\n", "      <td>49.0</td>\n", "      <td>7350.0</td>\n", "      <td>5110.0</td>\n", "      <td>1102500.0</td>\n", "      <td>766500.0</td>\n", "      <td>532900.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3793500160</th>\n", "      <td>323000.0</td>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>2.50</td>\n", "      <td>1890.0</td>\n", "      <td>6560.0</td>\n", "      <td>2.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>...</td>\n", "      <td>9.0</td>\n", "      <td>21.0</td>\n", "      <td>5670.0</td>\n", "      <td>0.0</td>\n", "      <td>49.0</td>\n", "      <td>13230.0</td>\n", "      <td>0.0</td>\n", "      <td>3572100.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>10 rows \u00d7 79 columns</p>\n", "</div>"], "text/plain": ["                price    0    1     2       3         4    5    6    7    8  \\\n", "id                                                                            \n", "7129300520   221900.0  1.0  3.0  1.00  1180.0    5650.0  1.0  0.0  0.0  3.0   \n", "6414100192   538000.0  1.0  3.0  2.25  2570.0    7242.0  2.0  0.0  0.0  3.0   \n", "5631500400   180000.0  1.0  2.0  1.00   770.0   10000.0  1.0  0.0  0.0  3.0   \n", "2487200875   604000.0  1.0  4.0  3.00  1960.0    5000.0  1.0  0.0  0.0  5.0   \n", "1954400510   510000.0  1.0  3.0  2.00  1680.0    8080.0  1.0  0.0  0.0  3.0   \n", "7237550310  1225000.0  1.0  4.0  4.50  5420.0  101930.0  1.0  0.0  0.0  3.0   \n", "1321400060   257500.0  1.0  3.0  2.25  1715.0    6819.0  2.0  0.0  0.0  3.0   \n", "2008000270   291850.0  1.0  3.0  1.50  1060.0    9711.0  1.0  0.0  0.0  3.0   \n", "2414600126   229500.0  1.0  3.0  1.00  1780.0    7470.0  1.0  0.0  0.0  3.0   \n", "3793500160   323000.0  1.0  3.0  2.50  1890.0    6560.0  2.0  0.0  0.0  3.0   \n", "\n", "            ...    68    69       70      71     72       73       74  \\\n", "id          ...                                                         \n", "7129300520  ...   9.0  21.0   3540.0     0.0   49.0   8260.0      0.0   \n", "6414100192  ...   9.0  21.0   6510.0  1200.0   49.0  15190.0   2800.0   \n", "5631500400  ...   9.0  18.0   2310.0     0.0   36.0   4620.0      0.0   \n", "2487200875  ...  25.0  35.0   5250.0  4550.0   49.0   7350.0   6370.0   \n", "1954400510  ...   9.0  24.0   5040.0     0.0   64.0  13440.0      0.0   \n", "7237550310  ...   9.0  33.0  11670.0  4590.0  121.0  42790.0  16830.0   \n", "1321400060  ...   9.0  21.0   5145.0     0.0   49.0  12005.0      0.0   \n", "2008000270  ...   9.0  21.0   3180.0     0.0   49.0   7420.0      0.0   \n", "2414600126  ...   9.0  21.0   3150.0  2190.0   49.0   7350.0   5110.0   \n", "3793500160  ...   9.0  21.0   5670.0     0.0   49.0  13230.0      0.0   \n", "\n", "                    75         76         77  \n", "id                                            \n", "7129300520   1392400.0        0.0        0.0  \n", "6414100192   4708900.0   868000.0   160000.0  \n", "5631500400    592900.0        0.0        0.0  \n", "2487200875   1102500.0   955500.0   828100.0  \n", "1954400510   2822400.0        0.0        0.0  \n", "7237550310  15132100.0  5951700.0  2340900.0  \n", "1321400060   2941225.0        0.0        0.0  \n", "2008000270   1123600.0        0.0        0.0  \n", "2414600126   1102500.0   766500.0   532900.0  \n", "3793500160   3572100.0        0.0        0.0  \n", "\n", "[10 rows x 79 columns]"]}, "execution_count": 53, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.preprocessing import PolynomialFeatures\n", "\n", "\n", "df = pd.read_csv('data/king_county.csv', index_col='id')\n", "\n", "pf = PolynomialFeatures(2)\n", "\n", "df_poly = pd.DataFrame(pf.fit_transform(df.drop('price', axis=1)))\n", "df_poly.index = df.index\n", "df_poly['price'] = df['price']\n", "\n", "cols = list(df_poly)\n", "# move the column to head of list using index, pop and insert\n", "cols.insert(0, cols.pop(cols.index('price')))\n", "\n", "df_poly = df_poly.loc[:,cols]\n", "\n", "df_poly.head(10)"]}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [], "source": ["np.random.seed(42)\n", "r_2 = []\n", "complex_rmse = []\n", "for i in range(100):\n", "    \n", "    df_sample = df_poly.sample(1000, replace=True)\n", "    y = df_sample.price\n", "    X = df_sample.drop('price', axis=1)\n", "    \n", "    lr = LinearRegression()\n", "    lr.fit(X, y)\n", "    y_hat = lr.predict(X)\n", "    complex_rmse.append(np.sqrt(mean_squared_error(y, y_hat)))\n", "    r_2.append(lr.score(X,y))\n", "    "]}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["simpl mean 228303.4528251759\n", "compl mean 195246.84153558695\n", "simp variance 73238189.28935923\n", "comp variance 1092553181.415185\n"]}], "source": ["print(f'simpl mean {np.mean(simple_rmse)}')\n", "print(f'compl mean {np.mean(complex_rmse)}')\n", "\n", "print(f'simp variance {np.var(simple_rmse)}')\n", "print(f'comp variance {np.var(complex_rmse)}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![which_model](img/which_model_is_better_2.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 5. Train Test Split"]}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is hard to know if your model is too simple or complex by just using it on training data.\n", "\n", "We can hold out part of our training sample, and use it as a test sample and use it to monitor our prediction error.\n", "\n", "This allows us to evaluate whether our model has the right balance of bias/variance. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src='img/testtrainsplit.png' width =550 />"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* **training set** \u2014a subset to train a model.\n", "* **test set**\u2014a subset to test the trained model.\n"]}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(16209, 2)\n", "(5404, 2)\n", "True\n", "True\n"]}], "source": ["import pandas as pd\n", "df = pd.read_csv('data/king_county.csv', index_col='id')\n", "\n", "y = df.price\n", "X = df[['bedrooms', 'sqft_living']]\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = .25)\n", "\n", "print(X_train.shape)\n", "print(X_test.shape)\n", "\n", "print(X_train.shape[0] == y_train.shape[0])\n", "print(X_test.shape[0] == y_test.shape[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**How do we know if our model is overfitting or underfitting?**\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If our model is not performing well on the training  data, we are probably underfitting it.  \n", "\n", "\n", "To know if our  model is overfitting the data, we need  to test our model on unseen data. \n", "We then measure our performance on the unseen data. \n", "\n", "If the model performs way worse on the  unseen data, it is probably  overfitting the data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src='https://developers.google.com/machine-learning/crash-course/images/WorkflowWithTestSet.svg' width=500/>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Word Play in groups\n", "\n", "Fill in the variable to correctly finish the sentences.\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["\n", "b_or_v = 'add a letter'\n", "over_under = 'add a number'\n", "\n", "one = \"The model has a high R^2 on both the training set, but low on the test \" +  b_or_v + \" \" + over_under\n", "two = \"The model has a low RMSE on training and a low RMSE on test\" + b_or_v + \" \" + over_under\n", "three = \"The model performs well on data it is fit on and well on data it has not seen\" + b_or_v + \" \" + over_under\n", "seven = \"The model has high R^2 on the training set and low R^2 on the test\"  + b_or_v + \" \" + over_under\n", "four = \"The model leaves out many of the meaningful predictors, but is consistent across samples\" + b_or_v + \" \" + over_under\n", "five = \"The model is highly sensitive to random noise in the training set\"  + b_or_v + \" \" + over_under\n", "six = \"The model has a low R^2 on training but high on the test set\"  + b_or_v + \" \" + over_under\n", "\n", "\n", "a = \"The model has low bias and high variance.\"\n", "b = \"The model has high bias and low variance.\"\n", "c = \"The model has both low bias and variance\"\n", "d = \"The model has high bias and high variance\"\n", "\n", "over = \"In otherwords, it is overfit.\"\n", "under = \"In otherwords, it is underfit.\"\n", "other = 'That is an abberation'\n", "good = \"In otherwords, we have a solid model\"\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Should you ever fit on your test set?  \n", "\n", "\n", "![no](https://media.giphy.com/media/d10dMmzqCYqQ0/giphy.gif)\n", "\n", "\n", "**Never fit on test data.** If you are seeing surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set. \n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's go back to our KC housing data without the polynomial transformation."]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>price</th>\n", "      <th>bedrooms</th>\n", "      <th>bathrooms</th>\n", "      <th>sqft_living</th>\n", "      <th>sqft_lot</th>\n", "      <th>floors</th>\n", "      <th>waterfront</th>\n", "      <th>view</th>\n", "      <th>condition</th>\n", "      <th>grade</th>\n", "      <th>sqft_above</th>\n", "      <th>sqft_basement</th>\n", "    </tr>\n", "    <tr>\n", "      <th>id</th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>7129300520</th>\n", "      <td>221900.0</td>\n", "      <td>3</td>\n", "      <td>1.00</td>\n", "      <td>1180</td>\n", "      <td>5650</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>7</td>\n", "      <td>1180</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6414100192</th>\n", "      <td>538000.0</td>\n", "      <td>3</td>\n", "      <td>2.25</td>\n", "      <td>2570</td>\n", "      <td>7242</td>\n", "      <td>2.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>7</td>\n", "      <td>2170</td>\n", "      <td>400</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5631500400</th>\n", "      <td>180000.0</td>\n", "      <td>2</td>\n", "      <td>1.00</td>\n", "      <td>770</td>\n", "      <td>10000</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>6</td>\n", "      <td>770</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2487200875</th>\n", "      <td>604000.0</td>\n", "      <td>4</td>\n", "      <td>3.00</td>\n", "      <td>1960</td>\n", "      <td>5000</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>5</td>\n", "      <td>7</td>\n", "      <td>1050</td>\n", "      <td>910</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1954400510</th>\n", "      <td>510000.0</td>\n", "      <td>3</td>\n", "      <td>2.00</td>\n", "      <td>1680</td>\n", "      <td>8080</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>8</td>\n", "      <td>1680</td>\n", "      <td>0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["               price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n", "id                                                                         \n", "7129300520  221900.0         3       1.00         1180      5650     1.0   \n", "6414100192  538000.0         3       2.25         2570      7242     2.0   \n", "5631500400  180000.0         2       1.00          770     10000     1.0   \n", "2487200875  604000.0         4       3.00         1960      5000     1.0   \n", "1954400510  510000.0         3       2.00         1680      8080     1.0   \n", "\n", "            waterfront  view  condition  grade  sqft_above  sqft_basement  \n", "id                                                                         \n", "7129300520           0     0          3      7        1180              0  \n", "6414100192           0     0          3      7        2170            400  \n", "5631500400           0     0          3      6         770              0  \n", "2487200875           0     0          5      7        1050            910  \n", "1954400510           0     0          3      8        1680              0  "]}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": ["df = pd.read_csv('data/king_county.csv', index_col='id')\n", "\n", "#Date  is not in the correct format so we are dropping it for now.\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, we create a train-test split via the sklearn model selection package."]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "np.random.seed(42)\n", "\n", "y = df.price\n", "X = df[['bedrooms', 'sqft_living']]\n", "\n", "# Here is the convention for a traditional train-test split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43, test_size=.25)"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["# Instanstiate your linear regression object\n", "lr = LinearRegression()"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"data": {"text/plain": ["LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"]}, "execution_count": 30, "metadata": {}, "output_type": "execute_result"}], "source": ["# fit the model on the training set\n", "lr.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.5132349854445817"]}, "execution_count": 31, "metadata": {}, "output_type": "execute_result"}], "source": ["# Check the R^2 of the training data\n", "lr.score(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([-54632.9149931 ,    311.65365556])"]}, "execution_count": 32, "metadata": {}, "output_type": "execute_result"}], "source": ["lr.coef_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A .513 R-squared reflects a model that explains aabout half of the total variance in the data. \n", "\n", "### Knowledge check\n", "How would you describe the bias of the model based on the above training R^2?"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["# Your answer here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, we test how well the model performs on the unseen test data. Remember, we do not fit the model again. The model has calculated the optimal parameters learning from the training set.  \n"]}, {"cell_type": "code", "execution_count": 83, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.48688154021233165"]}, "execution_count": 83, "metadata": {}, "output_type": "execute_result"}], "source": ["lr.score(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The difference between the train and test scores are low.\n", "\n", "What does that indicate about variance?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Now, let's try the same thing with our complex, polynomial model."]}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>price</th>\n", "      <th>bedrooms</th>\n", "      <th>bathrooms</th>\n", "      <th>sqft_living</th>\n", "      <th>sqft_lot</th>\n", "      <th>floors</th>\n", "      <th>waterfront</th>\n", "      <th>view</th>\n", "      <th>condition</th>\n", "      <th>grade</th>\n", "      <th>sqft_above</th>\n", "      <th>sqft_basement</th>\n", "    </tr>\n", "    <tr>\n", "      <th>id</th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "      <th></th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>7129300520</th>\n", "      <td>221900.0</td>\n", "      <td>3</td>\n", "      <td>1.00</td>\n", "      <td>1180</td>\n", "      <td>5650</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>7</td>\n", "      <td>1180</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6414100192</th>\n", "      <td>538000.0</td>\n", "      <td>3</td>\n", "      <td>2.25</td>\n", "      <td>2570</td>\n", "      <td>7242</td>\n", "      <td>2.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>7</td>\n", "      <td>2170</td>\n", "      <td>400</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5631500400</th>\n", "      <td>180000.0</td>\n", "      <td>2</td>\n", "      <td>1.00</td>\n", "      <td>770</td>\n", "      <td>10000</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>6</td>\n", "      <td>770</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2487200875</th>\n", "      <td>604000.0</td>\n", "      <td>4</td>\n", "      <td>3.00</td>\n", "      <td>1960</td>\n", "      <td>5000</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>5</td>\n", "      <td>7</td>\n", "      <td>1050</td>\n", "      <td>910</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1954400510</th>\n", "      <td>510000.0</td>\n", "      <td>3</td>\n", "      <td>2.00</td>\n", "      <td>1680</td>\n", "      <td>8080</td>\n", "      <td>1.0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "      <td>8</td>\n", "      <td>1680</td>\n", "      <td>0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["               price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n", "id                                                                         \n", "7129300520  221900.0         3       1.00         1180      5650     1.0   \n", "6414100192  538000.0         3       2.25         2570      7242     2.0   \n", "5631500400  180000.0         2       1.00          770     10000     1.0   \n", "2487200875  604000.0         4       3.00         1960      5000     1.0   \n", "1954400510  510000.0         3       2.00         1680      8080     1.0   \n", "\n", "            waterfront  view  condition  grade  sqft_above  sqft_basement  \n", "id                                                                         \n", "7129300520           0     0          3      7        1180              0  \n", "6414100192           0     0          3      7        2170            400  \n", "5631500400           0     0          3      6         770              0  \n", "2487200875           0     0          5      7        1050            910  \n", "1954400510           0     0          3      8        1680              0  "]}, "execution_count": 57, "metadata": {}, "output_type": "execute_result"}], "source": ["df = pd.read_csv('data/king_county.csv', index_col='id')\n", "df.head()"]}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>0</th>\n", "      <th>1</th>\n", "      <th>2</th>\n", "      <th>3</th>\n", "      <th>4</th>\n", "      <th>5</th>\n", "      <th>6</th>\n", "      <th>7</th>\n", "      <th>8</th>\n", "      <th>9</th>\n", "      <th>...</th>\n", "      <th>354</th>\n", "      <th>355</th>\n", "      <th>356</th>\n", "      <th>357</th>\n", "      <th>358</th>\n", "      <th>359</th>\n", "      <th>360</th>\n", "      <th>361</th>\n", "      <th>362</th>\n", "      <th>363</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>1.00</td>\n", "      <td>1180.0</td>\n", "      <td>5650.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>7.0</td>\n", "      <td>...</td>\n", "      <td>343.0</td>\n", "      <td>57820.0</td>\n", "      <td>0.0</td>\n", "      <td>9746800.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>1.643032e+09</td>\n", "      <td>0.000000e+00</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>2.25</td>\n", "      <td>2570.0</td>\n", "      <td>7242.0</td>\n", "      <td>2.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>7.0</td>\n", "      <td>...</td>\n", "      <td>343.0</td>\n", "      <td>106330.0</td>\n", "      <td>19600.0</td>\n", "      <td>32962300.0</td>\n", "      <td>6076000.0</td>\n", "      <td>1120000.0</td>\n", "      <td>1.021831e+10</td>\n", "      <td>1.883560e+09</td>\n", "      <td>347200000.0</td>\n", "      <td>64000000.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>1.0</td>\n", "      <td>2.0</td>\n", "      <td>1.00</td>\n", "      <td>770.0</td>\n", "      <td>10000.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>6.0</td>\n", "      <td>...</td>\n", "      <td>216.0</td>\n", "      <td>27720.0</td>\n", "      <td>0.0</td>\n", "      <td>3557400.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>4.565330e+08</td>\n", "      <td>0.000000e+00</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>1.0</td>\n", "      <td>4.0</td>\n", "      <td>3.00</td>\n", "      <td>1960.0</td>\n", "      <td>5000.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>5.0</td>\n", "      <td>7.0</td>\n", "      <td>...</td>\n", "      <td>343.0</td>\n", "      <td>51450.0</td>\n", "      <td>44590.0</td>\n", "      <td>7717500.0</td>\n", "      <td>6688500.0</td>\n", "      <td>5796700.0</td>\n", "      <td>1.157625e+09</td>\n", "      <td>1.003275e+09</td>\n", "      <td>869505000.0</td>\n", "      <td>753571000.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>1.0</td>\n", "      <td>3.0</td>\n", "      <td>2.00</td>\n", "      <td>1680.0</td>\n", "      <td>8080.0</td>\n", "      <td>1.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>3.0</td>\n", "      <td>8.0</td>\n", "      <td>...</td>\n", "      <td>512.0</td>\n", "      <td>107520.0</td>\n", "      <td>0.0</td>\n", "      <td>22579200.0</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "      <td>4.741632e+09</td>\n", "      <td>0.000000e+00</td>\n", "      <td>0.0</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>5 rows \u00d7 364 columns</p>\n", "</div>"], "text/plain": ["   0    1     2       3        4    5    6    7    8    9    ...    354  \\\n", "0  1.0  3.0  1.00  1180.0   5650.0  1.0  0.0  0.0  3.0  7.0  ...  343.0   \n", "1  1.0  3.0  2.25  2570.0   7242.0  2.0  0.0  0.0  3.0  7.0  ...  343.0   \n", "2  1.0  2.0  1.00   770.0  10000.0  1.0  0.0  0.0  3.0  6.0  ...  216.0   \n", "3  1.0  4.0  3.00  1960.0   5000.0  1.0  0.0  0.0  5.0  7.0  ...  343.0   \n", "4  1.0  3.0  2.00  1680.0   8080.0  1.0  0.0  0.0  3.0  8.0  ...  512.0   \n", "\n", "        355      356         357        358        359           360  \\\n", "0   57820.0      0.0   9746800.0        0.0        0.0  1.643032e+09   \n", "1  106330.0  19600.0  32962300.0  6076000.0  1120000.0  1.021831e+10   \n", "2   27720.0      0.0   3557400.0        0.0        0.0  4.565330e+08   \n", "3   51450.0  44590.0   7717500.0  6688500.0  5796700.0  1.157625e+09   \n", "4  107520.0      0.0  22579200.0        0.0        0.0  4.741632e+09   \n", "\n", "            361          362          363  \n", "0  0.000000e+00          0.0          0.0  \n", "1  1.883560e+09  347200000.0   64000000.0  \n", "2  0.000000e+00          0.0          0.0  \n", "3  1.003275e+09  869505000.0  753571000.0  \n", "4  0.000000e+00          0.0          0.0  \n", "\n", "[5 rows x 364 columns]"]}, "execution_count": 68, "metadata": {}, "output_type": "execute_result"}], "source": ["poly_2 = PolynomialFeatures(3)\n", "\n", "X_poly = pd.DataFrame(\n", "            poly_2.fit_transform(df.drop('price', axis=1))\n", "                      )\n", "\n", "y = df.price\n", "X_poly.head()"]}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.7133044254532317"]}, "execution_count": 69, "metadata": {}, "output_type": "execute_result"}], "source": ["X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=20, test_size=.25)\n", "lr_poly = LinearRegression()\n", "\n", "# Always fit on the training set\n", "lr_poly.fit(X_train, y_train)\n", "\n", "lr_poly.score(X_train, y_train)"]}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [], "source": ["# That indicates a lower bias"]}, {"cell_type": "code", "execution_count": 71, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.6119026292718351"]}, "execution_count": 71, "metadata": {}, "output_type": "execute_result"}], "source": ["lr_poly.score(X_test, y_test)"]}, {"cell_type": "code", "execution_count": 72, "metadata": {}, "outputs": [], "source": ["# There is a large difference between train and test, showing high variance."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pair Exercise"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### [Link](https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data) about data leakage and scalars"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The link above explains that if you are going to scale your data, you should only train your scalar on the training data to prevent data leakage.  \n", "\n", "Perform the same train test split as shown aboe for the simple model, but now scale your data appropriately.  \n", "\n", "The R2 for both train and test should be the same.\n"]}, {"cell_type": "code", "execution_count": 147, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "np.random.seed(42)\n", "\n", "y = df.price\n", "X = df[['bedrooms', 'sqft_living']]\n", "\n", "# Train test split with random_state=43 and test_size=.25\n", "\n", "\n", "# Scale appropriately\n", "\n", "# fit and score the model \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Kfolds: Even More Rigorous Validation  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["For a more rigorous cross-validation, we turn to K-folds"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![kfolds](img/k_folds.png)\n", "\n", "[image via sklearn](https://scikit-learn.org/stable/modules/cross_validation.html)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this process, we split the dataset into train and test as usual, then we perform a shuffling train test split on the train set.  \n", "\n", "KFolds holds out one fraction of the dataset, trains on the larger fraction, then calculates a test score on the held out set.  It repeats this process until each group has served as the test set.\n", "\n", "We tune our parameters on the training set using kfolds, then validate on the test data.  This allows us to build our model and check to see if it is overfit without touching the test data set.  This protects our model from bias."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Fill in the Blank"]}, {"cell_type": "code", "execution_count": 73, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Maximilian\n"]}], "source": ["mccalister = ['Adam', 'Amanda','Chum', 'Dann', \n", " 'Jacob', 'Jason', 'Johnhoy', 'Karim', \n", "'Leana','Luluva', 'Matt', 'Maximilian','Syd' ]\n", "\n", "choice = np.random.choice(mccalister)\n", "print(choice)"]}, {"cell_type": "code", "execution_count": 165, "metadata": {}, "outputs": [], "source": ["X = df.drop('price', axis=1)\n", "y = df.price\n"]}, {"cell_type": "code", "execution_count": 75, "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'fill_in' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-75-0cd4b40a7ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'fill_in' is not defined"]}], "source": ["from sklearn.model_selection import KFold\n", "\n", "# Instantiate the KFold object\n", "kf = KFold(n_splits=5)\n", "\n", "train_r2 = []\n", "test_r2 = []\n", "\n", "# kf.split() splits the data via index\n", "for train_ind, test_ind in kf.split(X,y):\n", "    \n", "    X_train, y_train = fill_in, fill_in\n", "    X_test, y_test = fill_in, fill_in\n", "    \n", "    # fill in fit\n", "    \n", "    \n", "    train_r2.append(lr.score(X_train, y_train))\n", "    test_r2.append(lr.score(X_test, y_test))"]}, {"cell_type": "code", "execution_count": 80, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.5068353530791848"]}, "execution_count": 80, "metadata": {}, "output_type": "execute_result"}], "source": ["# Mean train r_2\n", "np.mean(train_r2)"]}, {"cell_type": "code", "execution_count": 81, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.5043547355695521"]}, "execution_count": 81, "metadata": {}, "output_type": "execute_result"}], "source": ["# Mean test r_2\n", "np.mean(test_r2)"]}, {"cell_type": "code", "execution_count": 82, "metadata": {}, "outputs": [], "source": ["# Test out our polynomial model\n", "poly_2 = PolynomialFeatures(2)\n", "\n", "df_poly = pd.DataFrame(\n", "            poly_2.fit_transform(df.drop('price', axis=1))\n", "                      )\n", "\n", "X = df_poly\n", "y = df.price\n"]}, {"cell_type": "code", "execution_count": 83, "metadata": {}, "outputs": [], "source": ["kf = KFold(n_splits=5)\n", "\n", "train_r2 = []\n", "test_r2 = []\n", "for train_ind, test_ind in kf.split(X,y):\n", "    \n", "    X_train, y_train = X.iloc[train_ind], y.iloc[train_ind]\n", "    X_test, y_test = X.iloc[test_ind], y.iloc[test_ind]\n", "    \n", "    lr.fit(X_train, y_train)\n", "    train_r2.append(lr.score(X_train, y_train))\n", "    test_r2.append(lr.score(X_test, y_test))"]}, {"cell_type": "code", "execution_count": 84, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.6954523534689443"]}, "execution_count": 84, "metadata": {}, "output_type": "execute_result"}], "source": ["# Mean train r_2\n", "np.mean(train_r2)"]}, {"cell_type": "code", "execution_count": 85, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.6606213729380702"]}, "execution_count": 85, "metadata": {}, "output_type": "execute_result"}], "source": ["# Mean test r_2\n", "np.mean(test_r2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once we have an acceptable model, we train our model on the entire training set, and score on the test to validate.\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 4}